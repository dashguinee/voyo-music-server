name: Audio Conquest

on:
  workflow_dispatch:
    inputs:
      chunk_size:
        description: 'Tracks per job'
        default: '5000'
        type: string
      total_jobs:
        description: 'Number of parallel jobs'
        default: '20'
        type: string

jobs:
  # First job: Get track list and create chunks
  prepare:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      total_tracks: ${{ steps.set-matrix.outputs.total_tracks }}
    steps:
      - name: Get tracks to process
        id: set-matrix
        run: |
          # Fetch track count from Supabase
          TOTAL=$(curl -s "https://anmgyxhnyhbyxzpjhxgx.supabase.co/rest/v1/video_intelligence?select=youtube_id&limit=1" \
            -H "apikey: ${{ secrets.SUPABASE_KEY }}" \
            -H "Prefer: count=exact" \
            -I | grep -i x-total-count | awk '{print $2}' | tr -d '\r')

          echo "Total tracks: $TOTAL"
          echo "total_tracks=$TOTAL" >> $GITHUB_OUTPUT

          # Create matrix of job indices
          NUM_JOBS=${{ inputs.total_jobs }}
          CHUNK_SIZE=${{ inputs.chunk_size }}

          # Generate matrix JSON
          MATRIX="["
          for i in $(seq 0 $((NUM_JOBS - 1))); do
            OFFSET=$((i * CHUNK_SIZE))
            if [ $i -gt 0 ]; then MATRIX="$MATRIX,"; fi
            MATRIX="$MATRIX{\"job_id\":$i,\"offset\":$OFFSET,\"limit\":$CHUNK_SIZE}"
          done
          MATRIX="$MATRIX]"

          echo "matrix={\"include\":$MATRIX}" >> $GITHUB_OUTPUT

  # Parallel download jobs
  download:
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 350
    strategy:
      fail-fast: false
      max-parallel: 20
      matrix: ${{ fromJson(needs.prepare.outputs.matrix) }}

    steps:
      - name: Setup
        run: |
          echo "Job ${{ matrix.job_id }}: Processing offset ${{ matrix.offset }}, limit ${{ matrix.limit }}"

          # Install yt-dlp
          sudo curl -L https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -o /usr/local/bin/yt-dlp
          sudo chmod a+rx /usr/local/bin/yt-dlp

          # Install Python deps
          pip install boto3 requests

          # Write YouTube cookies
          cat << 'COOKIES' > /tmp/cookies.txt
          # Netscape HTTP Cookie File
          .youtube.com	TRUE	/	TRUE	0	__Secure-3PAPISID	cZoTw5ODO1Kk0yzS/AMihDY2kCbzOkuhWO
          .youtube.com	TRUE	/	FALSE	0	PREF	hl=en&tz=UTC
          .youtube.com	TRUE	/	TRUE	0	SOCS	CAI
          .youtube.com	TRUE	/	TRUE	0	YSC	9F5X2bh-4dk
          .youtube.com	TRUE	/	TRUE	1782868418	__Secure-ROLLOUT_TOKEN	CNbns-X46frgpQEQ05610tbrkQMYku7j2tbrkQM%3D
          .youtube.com	TRUE	/	TRUE	1782892772	VISITOR_INFO1_LIVE	I56cNKMY8Jo
          .youtube.com	TRUE	/	TRUE	1782892772	VISITOR_PRIVACY_METADATA	CgJNWRIEGgAgXg%3D%3D
          .youtube.com	TRUE	/	TRUE	1767341027	GPS	1
          COOKIES

      - name: Create worker script
        run: |
          cat << 'SCRIPT' > worker.py
          import os
          import subprocess
          import time
          import random
          from pathlib import Path
          import boto3
          from botocore.config import Config
          import requests

          # Config from env
          SUPABASE_URL = "https://anmgyxhnyhbyxzpjhxgx.supabase.co"
          SUPABASE_KEY = os.environ['SUPABASE_KEY']

          R2_ACCOUNT_ID = os.environ['R2_ACCOUNT_ID']
          R2_ACCESS_KEY = os.environ['R2_ACCESS_KEY']
          R2_SECRET_KEY = os.environ['R2_SECRET_KEY']
          R2_BUCKET = 'voyo-audio'

          OFFSET = int(os.environ['OFFSET'])
          LIMIT = int(os.environ['LIMIT'])
          JOB_ID = os.environ['JOB_ID']

          TEMP_DIR = Path("/tmp/voyo-audio")
          TEMP_DIR.mkdir(exist_ok=True)

          # R2 client
          r2 = boto3.client(
              's3',
              endpoint_url=f'https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com',
              aws_access_key_id=R2_ACCESS_KEY,
              aws_secret_access_key=R2_SECRET_KEY,
              config=Config(retries={'max_attempts': 3})
          )

          def log(msg):
              print(f"[Job {JOB_ID}] {msg}", flush=True)

          def get_tracks():
              headers = {"apikey": SUPABASE_KEY, "Authorization": f"Bearer {SUPABASE_KEY}"}
              url = f"{SUPABASE_URL}/rest/v1/video_intelligence?select=youtube_id&limit={LIMIT}&offset={OFFSET}"
              resp = requests.get(url, headers=headers, timeout=60)
              resp.raise_for_status()
              return [t['youtube_id'] for t in resp.json() if t.get('youtube_id')]

          def get_existing():
              existing = set()
              paginator = r2.get_paginator('list_objects_v2')
              try:
                  for page in paginator.paginate(Bucket=R2_BUCKET, Prefix="128/"):
                      for obj in page.get('Contents', []):
                          key = obj['Key']
                          if '/' in key:
                              filename = key.split('/')[-1]
                              if '.' in filename:
                                  existing.add(filename.rsplit('.', 1)[0])
              except:
                  pass
              return existing

          def download(yt_id):
              output = TEMP_DIR / f"{yt_id}.opus"
              cmd = [
                  "yt-dlp", "-x", "--audio-format", "opus", "--audio-quality", "5",
                  "-o", str(TEMP_DIR / f"{yt_id}.%(ext)s"),
                  "--no-playlist", "--quiet", "--no-warnings",
                  "--cookies", "/tmp/cookies.txt",
                  "--retries", "2", "--socket-timeout", "30",
                  "--sleep-interval", "1", "--max-sleep-interval", "3",
                  f"https://www.youtube.com/watch?v={yt_id}"
              ]
              try:
                  subprocess.run(cmd, capture_output=True, timeout=90)
                  if output.exists():
                      return output
                  webm = TEMP_DIR / f"{yt_id}.webm"
                  if webm.exists():
                      webm.rename(output)
                      return output
              except:
                  pass
              return None

          def upload(path, yt_id):
              try:
                  r2.upload_file(str(path), R2_BUCKET, f"128/{yt_id}.opus")
                  r2.upload_file(str(path), R2_BUCKET, f"64/{yt_id}.opus")
                  return True
              except:
                  return False

          def main():
              log(f"Starting: offset={OFFSET}, limit={LIMIT}")

              tracks = get_tracks()
              log(f"Got {len(tracks)} tracks from DB")

              existing = get_existing()
              log(f"Found {len(existing)} already on R2")

              to_process = [t for t in tracks if t not in existing]
              log(f"Need to process: {len(to_process)}")

              uploaded = 0
              failed = 0

              for i, yt_id in enumerate(to_process):
                  time.sleep(random.uniform(0.5, 1.5))

                  path = download(yt_id)
                  if path and upload(path, yt_id):
                      uploaded += 1
                      try:
                          path.unlink()
                      except:
                          pass
                  else:
                      failed += 1

                  if (i + 1) % 50 == 0:
                      log(f"Progress: {i+1}/{len(to_process)} | ✅ {uploaded} | ❌ {failed}")

              log(f"DONE: ✅ {uploaded} | ❌ {failed}")

          if __name__ == "__main__":
              main()
          SCRIPT

      - name: Run worker
        env:
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          OFFSET: ${{ matrix.offset }}
          LIMIT: ${{ matrix.limit }}
          JOB_ID: ${{ matrix.job_id }}
        run: python worker.py
