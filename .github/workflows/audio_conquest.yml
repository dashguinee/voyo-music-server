name: Audio Conquest

on:
  workflow_dispatch:
    inputs:
      chunk_size:
        description: 'Tracks per job'
        default: '5000'
        type: string
      total_jobs:
        description: 'Number of parallel jobs (keep low to avoid cookie rotation)'
        default: '3'
        type: string
      start_offset:
        description: 'Starting offset (for parallel runs)'
        default: '0'
        type: string

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Get tracks to process
        id: set-matrix
        run: |
          NUM_JOBS=${{ inputs.total_jobs }}
          CHUNK_SIZE=${{ inputs.chunk_size }}
          START=${{ inputs.start_offset }}

          MATRIX="["
          for i in $(seq 0 $((NUM_JOBS - 1))); do
            OFFSET=$((START + i * CHUNK_SIZE))
            if [ $i -gt 0 ]; then MATRIX="$MATRIX,"; fi
            MATRIX="$MATRIX{\"job_id\":$i,\"offset\":$OFFSET,\"limit\":$CHUNK_SIZE}"
          done
          MATRIX="$MATRIX]"

          echo "Starting at offset $START with $NUM_JOBS jobs of $CHUNK_SIZE each"
          echo "matrix={\"include\":$MATRIX}" >> $GITHUB_OUTPUT

  download:
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 350
    strategy:
      fail-fast: false
      max-parallel: 20
      matrix: ${{ fromJson(needs.prepare.outputs.matrix) }}

    steps:
      - name: Setup
        run: |
          echo "Job ${{ matrix.job_id }}: offset=${{ matrix.offset }}, limit=${{ matrix.limit }}"
          # Install deno for yt-dlp signature solving (no ffmpeg needed - no conversion)
          curl -fsSL https://deno.land/install.sh | sh
          export DENO_INSTALL="$HOME/.deno"
          export PATH="$DENO_INSTALL/bin:$PATH"
          echo "$DENO_INSTALL/bin" >> $GITHUB_PATH
          # Install yt-dlp
          sudo curl -L https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -o /usr/local/bin/yt-dlp
          sudo chmod a+rx /usr/local/bin/yt-dlp
          pip install boto3 requests
          # Cookies required - GitHub IPs are flagged by YouTube
          echo "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" | base64 -d > /tmp/cookies.txt

      - name: Create worker script
        run: |
          cat << 'SCRIPT' > worker.py
          import os
          import subprocess
          import time
          import random
          from pathlib import Path
          import boto3
          from botocore.config import Config
          import requests
          from concurrent.futures import ThreadPoolExecutor, as_completed

          SUPABASE_URL = "https://anmgyxhnyhbyxzpjhxgx.supabase.co"
          SUPABASE_KEY = os.environ['SUPABASE_KEY']
          R2_ACCOUNT_ID = os.environ['R2_ACCOUNT_ID']
          R2_ACCESS_KEY = os.environ['R2_ACCESS_KEY']
          R2_SECRET_KEY = os.environ['R2_SECRET_KEY']
          R2_BUCKET = 'voyo-audio'
          OFFSET = int(os.environ['OFFSET'])
          LIMIT = int(os.environ['LIMIT'])
          JOB_ID = os.environ['JOB_ID']

          TEMP_DIR = Path("/tmp/voyo-audio")
          TEMP_DIR.mkdir(exist_ok=True)

          r2 = boto3.client('s3',
              endpoint_url=f'https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com',
              aws_access_key_id=R2_ACCESS_KEY,
              aws_secret_access_key=R2_SECRET_KEY,
              config=Config(retries={'max_attempts': 3}))

          def log(msg):
              print(f"[Job {JOB_ID}] {msg}", flush=True)

          def get_tracks():
              headers = {"apikey": SUPABASE_KEY, "Authorization": f"Bearer {SUPABASE_KEY}"}
              all_tracks = []
              current_offset = OFFSET
              remaining = LIMIT
              # Supabase max 1000 per request - paginate
              while remaining > 0:
                  batch_size = min(1000, remaining)
                  url = f"{SUPABASE_URL}/rest/v1/video_intelligence?select=youtube_id&limit={batch_size}&offset={current_offset}"
                  resp = requests.get(url, headers=headers, timeout=60)
                  resp.raise_for_status()
                  batch = [t['youtube_id'] for t in resp.json() if t.get('youtube_id')]
                  if not batch:
                      break
                  all_tracks.extend(batch)
                  current_offset += len(batch)
                  remaining -= len(batch)
              return all_tracks

          def get_existing():
              existing = set()
              try:
                  paginator = r2.get_paginator('list_objects_v2')
                  # Check raw/ and legacy folders
                  for prefix in ["raw/", "audio/", "128/", "audio/128/"]:
                      for page in paginator.paginate(Bucket=R2_BUCKET, Prefix=prefix):
                          for obj in page.get('Contents', []):
                              key = obj['Key']
                              if '/' in key:
                                  filename = key.split('/')[-1]
                                  if '.' in filename:
                                      existing.add(filename.rsplit('.', 1)[0])
              except Exception as e:
                  log(f"Warning getting existing: {e}")
              return existing

          def download_and_upload(yt_id, debug=False):
              # BEST QUALITY: bestaudio with fallback, sorted by bitrate, no conversion
              cmd = ["yt-dlp", "-f", "bestaudio/best", "-S", "+size,+br", "-x",
                  "-o", str(TEMP_DIR / f"{yt_id}.%(ext)s"),
                  "--no-playlist",
                  "--cookies", "/tmp/cookies.txt",
                  "--retries", "2", "--socket-timeout", "30",
                  f"https://www.youtube.com/watch?v={yt_id}"]
              try:
                  result = subprocess.run(cmd, capture_output=True, timeout=120, text=True)
                  if debug or result.returncode != 0:
                      log(f"yt-dlp exit={result.returncode} stderr={result.stderr[:200] if result.stderr else 'none'}")

                  # Find whatever file was created (could be .opus, .m4a, .webm, etc.)
                  for ext in ['opus', 'webm', 'm4a', 'mp3', 'ogg']:
                      output = TEMP_DIR / f"{yt_id}.{ext}"
                      if output.exists():
                          # Upload to raw/ with original extension
                          r2.upload_file(str(output), R2_BUCKET, f"raw/{yt_id}.{ext}")
                          output.unlink()
                          return True
              except Exception as e:
                  log(f"Exception for {yt_id}: {e}")
              return False

          def main():
              log(f"Starting: offset={OFFSET}, limit={LIMIT}")
              tracks = get_tracks()
              log(f"Got {len(tracks)} tracks from DB")

              existing = get_existing()
              log(f"Found {len(existing)} already on R2")

              to_process = [t for t in tracks if t not in existing]
              log(f"Need to process: {len(to_process)}")

              if not to_process:
                  log("Nothing to process!")
                  return

              uploaded = 0
              failed = 0

              # Process with delays to avoid rate limits and cookie rotation
              for i, yt_id in enumerate(to_process):
                  time.sleep(random.uniform(1.0, 3.0))  # Longer delay - safer for cookies
                  # Debug first 3 tracks to see what's happening
                  if download_and_upload(yt_id, debug=(i < 3)):
                      uploaded += 1
                  else:
                      failed += 1
                  if (i + 1) % 25 == 0:
                      log(f"Progress: {i+1}/{len(to_process)} | ✅ {uploaded} | ❌ {failed}")

              log(f"DONE: ✅ {uploaded} | ❌ {failed}")

          if __name__ == "__main__":
              main()
          SCRIPT

      - name: Run worker
        env:
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          OFFSET: ${{ matrix.offset }}
          LIMIT: ${{ matrix.limit }}
          JOB_ID: ${{ matrix.job_id }}
        run: python worker.py
